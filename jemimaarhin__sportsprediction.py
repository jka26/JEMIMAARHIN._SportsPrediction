# -*- coding: utf-8 -*-
"""JEMIMAARHIN._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ketfNwafoW7VBNA5NQnZz_-hzCxaCm0v
"""

import pandas as pd
import numpy as np
from google.colab import drive

"""Loading the datasets"""

drive. mount('/content/drive')
#Training Data
players_23 = pd.read_csv('/content/drive/MyDrive/male_players (legacy).csv')
players_23

players_23.describe()

#Checking the number of missing values on each column in the two datasets
print(players_23.isnull().sum())

"""**#1 Data Preprocessing (Cleaning)**"""

new_players_23 = players_23.drop_duplicates()
new_players_23

L=[]
L_less = []
for i in new_players_23.columns:
    if((new_players_23[i].isnull().sum()) < (0.3 * new_players_23.shape[0])):
        L.append(i)
    else:
        L_less.append(i)

new_players_23 = new_players_23[L]
new_players_23

#checking if the columns with more than 30% missing values have been dropped
print(new_players_23.isnull().sum())

#Dropping columns using df.drop(df.loc)
#They are being dropped because they won't be needed in the prediction of the player's overall rating

new_players_23.drop(new_players_23.loc[:, 'player_url':'fifa_update_date'].columns, axis=1, inplace= True)
new_players_23.drop(new_players_23.loc[:, 'ls':'player_face_url'].columns, axis=1, inplace= True)
new_players_23

"""### Imputation & Encoding"""

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

#new_players_23.fillna(method= 'bfill')
imputer = SimpleImputer(strategy="median")

numeric_data= new_players_23.select_dtypes(include=np.number)
non_numeric= new_players_23.select_dtypes(include= ['object'])
numeric_data

non_numeric

imputer.fit(numeric_data)

X = imputer.transform(numeric_data)

players_23_tr = pd.DataFrame(X, columns=numeric_data.columns)
players_23_tr

#Encoding the categorical variables
lbl_encoders = {}
for col in non_numeric:
    label_encoder = LabelEncoder()
    non_numeric[col] = label_encoder.fit_transform(non_numeric[col])
    lbl_encoders[col] = label_encoder

non_numeric

new_df = pd.concat([players_23_tr, non_numeric], axis=1)
new_df

"""Feature Extraction"""

#The variable we are predicting is the overall rating so it will be dropped from the dataframe to be effectively predicted

y= new_df['overall']
x= new_df.drop('overall', axis=1)

#dataframe without the independent variable
x_df = pd.DataFrame(x)
x_df

"""EDA"""

#Finding the correlation between all the variables
corr_matrix = new_df.corr()
corr_matrix

#This is the correlation needed because it helps to see how much all the other variables are related to the dependemt variable (overall)
correlation = x_df.corrwith(y)
correlation

"""**#2 Feature Engineering**"""

important_features = correlation[(correlation < -0.5) | (correlation > 0.5)]
important_features

#Creating a dataframe for the important features
important_df = new_df[important_features.index]
important_df

from sklearn.preprocessing import StandardScaler

#Scaling the values of the important features
scale= StandardScaler()
scaled= scale.fit_transform(important_df)
scaled

"""**#3 Training Models**"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

xtrain,xtest,ytrain,ytest=train_test_split(scaled,y,test_size=0.2, random_state =42)

xtrain.shape

#Training a RandomForestRegressor

from sklearn.ensemble import RandomForestRegressor
import pickle as pkl

rf= RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True) #instantiation
rf.fit(xtrain, ytrain)  #fitting the model

rf_y_pred = rf.predict(xtest)

# Training an XGBoost Regressor

import xgboost as xg
xgb_r = xg.XGBRegressor(objective ='reg:linear', n_estimators = 10, seed = 123) #instantiation
xgb_r.fit(xtrain, ytrain) #fitting the model

xgb_y_pred = xgb_r.predict(xtest)

# Training a Gradient Boost Regressor

from sklearn.ensemble import GradientBoostingRegressor
SEED= 23
gbr = GradientBoostingRegressor(loss='absolute_error',
                                learning_rate=0.1,
                                n_estimators=300,
                                max_depth = 1,
                                random_state = SEED,
                                max_features = 5) #instantiating the Gradient Boosting Regressor

gbr.fit(xtrain, ytrain) #Fit to training set

gbr_y_pred = gbr.predict(xtest)

#Fine-tuning models
#RandomForestRegressor
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2'],
    'bootstrap': [True, False]
}

# Instantiate the model
rf = RandomForestRegressor(random_state=0, oob_score=True)

# Set up GridSearchCV
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error',
                           n_jobs=-1, verbose=1)

# Fit the GridSearchCV object to the training data
grid_search_rf.fit(xtrain, ytrain)

# Extract the best parameters and best model
best_params_rf = grid_search_rf.best_params_
best_model_rf = grid_search_rf.best_estimator_
print("Best parameters found: ", best_params_rf)

# Use the best model to make predictions
rf_y_pred2 = best_model_rf.predict(xtest)


#XGBoost Regressor
# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

xgb_r = xg.XGBRegressor(objective='reg:squarederror', seed=123)  #instantiate

# Set up GridSearchCV
grid_search_xg = GridSearchCV(estimator=xgb_r, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error',
                           n_jobs=-1, verbose=1)

# Fit the GridSearchCV object to the training data to fine tune the model
grid_search_xg.fit(xtrain, ytrain)

# Extract the best parameters and best model
best_params_xg = grid_search_xg.best_params_
best_model_xg = grid_search_xg.best_estimator_
print("Best parameters found: ", best_params_xg)
xgb_y_pred = best_model_xg.predict(xtest)


#Gradient Boosting
# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [1, 3, 5],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 0.9, 1.0],
    'max_features': [None, 'auto', 'sqrt', 'log2']
}

# Instantiate the model
SEED = 23
gbr = GradientBoostingRegressor(loss='absolute_error', random_state=SEED)

# Set up GridSearchCV
grid_search_gb = GridSearchCV(estimator=gbr, param_grid=param_grid,
                           cv=5, scoring='neg_mean_squared_error',
                           n_jobs=-1, verbose=1)

# Fit the GridSearchCV object to the training data
grid_search_gb.fit(xtrain, ytrain)

# Extract the best parameters and best model
best_params_gb = grid_search_gb.best_params_
best_model_gb = grid_search_gb.best_estimator_
print("Best parameters found: ", best_params_gb)

# Use the best model to make predictions
gbr_y_pred = best_model_gb.predict(xtest)

"""**#4 Evaluation**"""

from sklearn.metrics import mean_absolute_error, mean_squared_log_error

# Evaluating the Random Forest Regressor

# Access the OOB Score
oob_score = rf.oob_score_
print(f'Out-of-Bag Score: {oob_score}')

#Using the MAE (Mean Absolute Error) and the RMSE (Root Mean Squared Error) as metrics to measure the performance
print(f"""Mean Absolute Error={mean_absolute_error(ytest, rf_y_pred)},
          Root Mean Squared Error={np.sqrt(mean_squared_log_error(ytest, rf_y_pred))}
          """)

# Evaluating the XGBoost Regressor

#Using the MAE (Mean Absolute Error) and the RMSE (Root Mean Squared Error) as metrics to measure the performance
print(f"""Mean Absolute Error={mean_absolute_error(ytest, xgb_y_pred)},
          Root Mean Squared Error={np.sqrt(mean_squared_log_error(ytest, xgb_y_pred))}
          """)

# Evaluating the Gradient Boost Regressor

#Using the MAE (Mean Absolute Error) and the RMSE (Root Mean Squared Error) as metrics to measure the performance
print(f"""Mean Absolute Error={mean_absolute_error(ytest, gbr_y_pred)},
          Root Mean Squared Error={np.sqrt(mean_squared_log_error(ytest, gbr_y_pred))}
          """)

"""Testing Data Preparation(Cleaning, Imputation and Encoding, Feature Extraction)"""

#Loading the testing data
players_22 = pd.read_csv('/content/drive/My Drive/players_22.csv')
players_22

new_players_22 = players_22.drop_duplicates()
new_players_22

new_players_22.describe()

new_players_22.info()

#Handling missing values in the testing data
L1=[]
L_less_1 = []
for i in new_players_22.columns:
    if((new_players_22[i].isnull().sum()) < (0.3 * new_players_22.shape[0])):
        L1.append(i)
    else:
        L_less_1.append(i)

new_players_22 = new_players_22[L1]
new_players_22

#checking if the columns with more than 30% missing values have been dropped
print(players_22.isnull().sum())

#Dropping irrelevant columns
#They are being dropped because they won't be needed in the prediction of the player's overall rating

new_players_22.drop(['player_url'], axis=1, inplace= True)
new_players_22.drop(new_players_22.loc[:, 'ls':'nation_flag_url'].columns, axis=1, inplace= True)
new_players_22

numeric= new_players_22.select_dtypes(include=np.number)
non_numeric_data= new_players_22.select_dtypes(include= ['object'])
numeric

non_numeric_data

X = imputer.fit_transform(numeric)

players_22_tr = pd.DataFrame(X, columns=numeric.columns)
players_22_tr

#Encoding the categorical variables
label_enc = {}
for i in non_numeric_data:
    label_encoder = LabelEncoder()
    non_numeric_data[i] = label_encoder.fit_transform(non_numeric_data[i])
    label_enc[i] = label_encoder

non_numeric_data

test_df = pd.concat([players_22_tr, non_numeric_data], axis=1)
test_df

Y = test_df['overall']
X = test_df.drop('overall', axis=1)

X_df = pd.DataFrame(X)
X_df

test_correlation = X_df.corrwith(Y)
test_correlation

test_features = test_correlation[(test_correlation < -0.5) | (test_correlation > 0.5)]
test_features

tfeatures_df = test_df[test_features.index]
tfeatures_df.drop(['release_clause_eur'], axis=1, inplace=True)
tfeatures_df.drop(['attacking_short_passing'], axis=1, inplace=True)
tfeatures_df.drop(['power_shot_power'], axis=1, inplace=True)
tfeatures_df.drop(['mentality_vision'], axis=1, inplace=True)
tfeatures_df

#Scaling the values for the relevant test features
scaled_test= scale.fit_transform(tfeatures_df)
scaled_test

"""**#5 Testing with New Dataset**"""

#The RandomForest Regressor is being used to test the new data since it was found to be the best performing model
rf_y_pred2= rf.predict(scaled_test)

#Measuring the performance of the RandomForest Regressor on the new data
mae_RF= mean_absolute_error(Y, rf_y_pred2 )
print(f"Mean Absolute Error (MAE): {mae_RF}")

#Saving the model in a pickle file
import pickle as pkl
rf_model = rf

# Specifying the filename for the pickle file
pickle_filename = "C:\\Users\\jarhi\\OneDrive - Ashesi University\\Documents\\rf_model.pkl"

# Saving
with open(pickle_filename, 'wb') as file:
    pkl.dump(rf_model, file)

"""**#6 Deployment**"""

!pip install streamlit

#Designing the fundamental view of the web page
import streamlit as st
from sklearn import preprocessing

model = pkl.load(open(pickle_filename, 'rb'))
cols=['potential', 'value_eur', 'wage_eur', 'passing', 'dribbling', 'movement_reactions', 'mentality_composure']

def main():
    st.title("Player Overall Rating Predictor")
    html_temp = """
    <div style="background:#025246 ;padding:10px">
    <h2 style="color:white;text-align:center;">Player Overall Rating Prediction App </h2>
    </div>
    """
    st.markdown(html_temp, unsafe_allow_html = True)

    potential = st.text_input("Potential", "0")
    value_eur = st.selectbox("Player Value", "0")
    wage_eur = st.selectbox("Wage", "0")
    passing = st.selectbox("Passing ability", "0")
    dribbling = st.selectbox("Dribbling ability", "0")
    movement_reactions = st.selectbox("Movement Reactions", "0")
    mentality_composure = st.selectbox("Mentality Composure", "0")

    if st.button("Predict"):
        features = [[potential, value_eur, wage_eur, passing, dribbling, movement_reactions, mentality_composure]]
        data = {'potential': potential, 'value_eur': value_eur, 'wage_eur': wage_eur, 'passing': passing, 'dribbling': dribbling, 'movement_reactions': movement_reactions, 'mentality_composure': mentality_composure}
        print(data)
        df=pd.DataFrame([list(data.values())], columns=['potential', 'value_eur', 'wage_eur', 'passing', 'dribbling', 'movement_reactions', 'mentality_composure'])

        features_list = df.values.tolist()
        prediction = model.predict(features_list)

        output = int(prediction[0])
        if output == 1:
            text = ">50K"
        else:
            text = "<=50K"

        st.success('Player Overall Rating is {}'.format(text))

if __name__=='__main__':
    main()



